{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b7cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, HTML\n",
    "import preprocess as p\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import f\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from math import isnan\n",
    "import warnings\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.impute import KNNImputer\n",
    "from impyute.imputation.cs import mice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec979622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path, path_prefix='data/', train=True):\n",
    "    dataset = 'train/' if train else 'test/'\n",
    "    file_list = os.listdir(path_prefix + dataset)\n",
    "    all_patients = []\n",
    "    for i,file in tqdm(enumerate(file_list)):\n",
    "        df_all = pd.read_csv(path_prefix + dataset + file, sep='|')\n",
    "        all_patients.append(df_all)\n",
    "    return all_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf0d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_name):\n",
    "    import pickle\n",
    "    with open(f\"{model_name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d14ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cd9cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df, rows_to_include=1):\n",
    "    \n",
    "    def stats(df,rows_to_include, future=6):\n",
    "        is_sick = bool(len(df[df.SepsisLabel == 1]))\n",
    "        if is_sick:\n",
    "            index = df[df.SepsisLabel == 1].index[0]\n",
    "            df = df[:index + future]\n",
    "        df = df.tail(rows_to_include + future).mean(axis=0)\n",
    "        return df\n",
    "    \n",
    "    labels = df.groupby(\"level_0\").apply(lambda x: len(set(x.SepsisLabel) & set([1]))).rename(\"level_1\")\n",
    "    df = df.merge(labels, on='level_0')\n",
    "    df['label'] = df['level_1']\n",
    "    df = df.drop(\"level_1\", axis=1)\n",
    "    df_last_row = df.groupby(\"level_0\").apply(lambda x: stats(x, rows_to_include, future = 1)).drop([\"level_0\", 'SepsisLabel'], axis=1)\n",
    "    return df_last_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4149730",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imp_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num_of_rows \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m]:\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimp_type = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimp_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, num_of_rows = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_of_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m     train_last_row \u001b[38;5;241m=\u001b[39m prepare_df(train, rows_to_include\u001b[38;5;241m=\u001b[39mnum_of_rows)\n\u001b[0;32m      5\u001b[0m     test_last_row \u001b[38;5;241m=\u001b[39m prepare_df(test, rows_to_include\u001b[38;5;241m=\u001b[39mnum_of_rows)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'imp_type' is not defined"
     ]
    }
   ],
   "source": [
    "for num_of_rows in [1,5, 10]:\n",
    "    \n",
    "    print(f\"imp_type = {imp_type}, num_of_rows = {num_of_rows}\")\n",
    "    train_last_row = prepare_df(train, rows_to_include=num_of_rows)\n",
    "    test_last_row = prepare_df(test, rows_to_include=num_of_rows)\n",
    "    print(\"Datasets Created\")\n",
    "    def objective(trial):\n",
    "        \"\"\"Define the objective function\"\"\"\n",
    "\n",
    "        params = {\n",
    "            'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 1.0),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n",
    "            'subsample': trial.suggest_loguniform('subsample', 0.01, 1.0),\n",
    "            'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.01, 1.0),\n",
    "            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0),\n",
    "            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0),\n",
    "            'scale_pos_weight':trial.suggest_int('scale_pos_weight', 1, 10),\n",
    "            'eval_metric': 'mlogloss',\n",
    "            'use_label_encoder': False\n",
    "        }\n",
    "\n",
    "    #     train_df = pd.concat([train_last_row, test_last_row], axis=0)\n",
    "        train_df = train_last_row\n",
    "        test_df = test_last_row\n",
    "\n",
    "        # Split the train dataset into features and labels\n",
    "        X_train = train_df.drop(\"label\", axis=1)\n",
    "        y_train = train_df[\"label\"]\n",
    "        X_test = test_df.drop(\"label\", axis=1)\n",
    "        y_test = test_df[\"label\"]\n",
    "\n",
    "        # Fit the model\n",
    "        optuna_model = XGBClassifier(**params)\n",
    "        optuna_model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = optuna_model.predict(X_test)\n",
    "\n",
    "        # Evaluate predictions\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        return f1\n",
    "\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=300)\n",
    "\n",
    "\n",
    "\n",
    "    train_last_row = prepare_df(train, fill_nulls=imp_type, rows_to_include=num_of_rows)\n",
    "    test_last_row = prepare_df(test, fill_nulls=imp_type, rows_to_include=num_of_rows, train_flag=False)\n",
    "    train_df = train_last_row\n",
    "    test_df = test_last_row\n",
    "\n",
    "\n",
    "    X_train = train_df.drop(\"label\", axis=1)\n",
    "    y_train = train_df[\"label\"]\n",
    "    X_test = test_df.drop(\"label\", axis=1)\n",
    "    y_test = test_df[\"label\"]\n",
    "\n",
    "    # Fit the model\n",
    "    model = XGBClassifier(**study.best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    from xgboost import plot_importance\n",
    "    plot_importance(model, max_num_features=10) # top 10 most important features\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c9cb45b6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-07 15:51:06,647]\u001b[0m A new study created in memory with name: no-name-3cc491bb-0e55-45cc-af90-7289ecafdc77\u001b[0m\n",
      "\u001b[33m[W 2023-05-07 15:51:06,671]\u001b[0m Trial 0 failed with parameters: {'n_estimators': 750, 'max_depth': 8, 'min_samples_split': 7, 'min_samples_leaf': 5, 'max_features': 'sqrt'} because of the following error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float32').\").\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ofekg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\ofekg\\AppData\\Local\\Temp\\ipykernel_25700\\1431491153.py\", line 38, in objective\n",
      "    clf.fit(X_train, y_train)\n",
      "  File \"C:\\Users\\ofekg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 327, in fit\n",
      "    X, y = self._validate_data(\n",
      "  File \"C:\\Users\\ofekg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\ofekg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 964, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"C:\\Users\\ofekg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"C:\\Users\\ofekg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\u001b[33m[W 2023-05-07 15:51:06,686]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [91]\u001b[0m, in \u001b[0;36m<cell line: 54>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Run hyperparameter optimization\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Print best hyperparameters and objective value\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters: \u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\optuna\\study\\study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Input \u001b[1;32mIn [91]\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     26\u001b[0m clf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\n\u001b[0;32m     27\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39mn_estimators,\n\u001b[0;32m     28\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39mmax_depth,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Fit classifier to training data\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Predict on validation data\u001b[39;00m\n\u001b[0;32m     41\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:327\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 327\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:964\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 964\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    979\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric)\n\u001b[0;32m    981\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:800\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    796\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    797\u001b[0m         )\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 800\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    803\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:114\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    108\u001b[0m         allow_nan\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misinf(X)\u001b[38;5;241m.\u001b[39many()\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(X)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m    112\u001b[0m     ):\n\u001b[0;32m    113\u001b[0m         type_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfinity\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_nan \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN, infinity\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    115\u001b[0m             msg_err\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    116\u001b[0m                 type_err, msg_dtype \u001b[38;5;28;01mif\u001b[39;00m msg_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    117\u001b[0m             )\n\u001b[0;32m    118\u001b[0m         )\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "X_train = train_last_row.drop(\"label\", axis=1)\n",
    "y_train = train_last_row[\"label\"]\n",
    "X_test = test_last_row.drop(\"label\", axis=1)\n",
    "y_test = test_last_row[\"label\"]\n",
    "\n",
    "\n",
    "# Split data into training and validation sets\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to be tuned\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 1050, step=100)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 32, log=True)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "    max_features = trial.suggest_categorical(\"max_features\", [\"auto\", \"sqrt\", \"log2\"])\n",
    "\n",
    "    class_weights = {0: 1, 1: 5}\n",
    "\n",
    "    # Define random forest classifier with hyperparameters\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        random_state=42,\n",
    "        class_weight=class_weights,\n",
    "    )\n",
    "\n",
    "    # Fit classifier to training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on validation data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Return accuracy score as objective value\n",
    "    return f1\n",
    "\n",
    "\n",
    "# Define study object\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "# Run hyperparameter optimization\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print best hyperparameters and objective value\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "print(\"Best f1: \", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f40ddd8",
   "metadata": {},
   "source": [
    "# With 5 rows back, best features were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7eaada1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_row_back = {'max_depth': 5, 'learning_rate': 0.12637669071933097, 'n_estimators': 199, 'min_child_weight': 7, 'gamma': 8.788101397006915e-06, 'subsample': 0.34961859437458515, 'colsample_bytree': 0.7460989708274746, 'reg_alpha': 1.09797444756061e-06, 'reg_lambda': 3.0861484148146835e-06, 'scale_pos_weight': 2}\n",
    "five_rows_back = {'max_depth': 6, 'learning_rate': 0.020021827800040026, 'n_estimators': 409, 'min_child_weight': 8, 'gamma': 0.002376046358768467, 'subsample': 0.3267777415133442, 'colsample_bytree': 0.9922434396349483, 'reg_alpha': 0.03345778334794334, 'reg_lambda': 0.00023611208593944376, 'scale_pos_weight': 2}\n",
    "ten_rows_back = {'max_depth': 4, 'learning_rate': 0.036332387280166745, 'n_estimators': 500, 'min_child_weight': 4, 'gamma': 3.7763230108728382e-06, 'subsample': 0.7222965801713592, 'colsample_bytree': 0.8140034993675944, 'reg_alpha': 2.0147933854605774e-07, 'reg_lambda': 4.2754835795019617e-07, 'scale_pos_weight': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57150543",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_row_back = {'max_depth': 5, 'learning_rate': 0.12637669071933097, 'n_estimators': 199, 'min_child_weight': 7, 'gamma': 8.788101397006915e-06, 'subsample': 0.34961859437458515, 'colsample_bytree': 0.7460989708274746, 'reg_alpha': 1.09797444756061e-06, 'reg_lambda': 3.0861484148146835e-06, 'scale_pos_weight': 2}\n",
    "five_rows_back = {'max_depth': 6, 'learning_rate': 0.020021827800040026, 'n_estimators': 409, 'min_child_weight': 8, 'gamma': 0.002376046358768467, 'subsample': 0.3267777415133442, 'colsample_bytree': 0.9922434396349483, 'reg_alpha': 0.03345778334794334, 'reg_lambda': 0.00023611208593944376, 'scale_pos_weight': 2}\n",
    "ten_rows_back = {'max_depth': 4, 'learning_rate': 0.036332387280166745, 'n_estimators': 500, 'min_child_weight': 4, 'gamma': 3.7763230108728382e-06, 'subsample': 0.7222965801713592, 'colsample_bytree': 0.8140034993675944, 'reg_alpha': 2.0147933854605774e-07, 'reg_lambda': 4.2754835795019617e-07, 'scale_pos_weight': 3}\n",
    "for rows, params in [(1, one_row_back), (5, five_rows_back), (10, ten_rows_back)]:\n",
    "    train_last_row = prepare_df(train, rows_to_include=rows)\n",
    "    test_last_row = prepare_df(test, rows_to_include=rows)\n",
    "\n",
    "    X_train = train_last_row.drop(\"label\", axis=1)\n",
    "    y_train = train_last_row[\"label\"]\n",
    "    X_test = test_last_row.drop(\"label\", axis=1)\n",
    "    y_test = test_last_row[\"label\"]\n",
    "\n",
    "    # Fit the model\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    save_model(model, f\"xgboost_{rows}_rows_back\")\n",
    "#     from xgboost import plot_importance\n",
    "#     plot_importance(model, max_num_features=10) # top 10 most important features\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a32b90",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a813583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "def impute_null_values(train_df, test_df=None, method='mean', k=1):\n",
    "    \"\"\"\n",
    "    Function that imputes null values of a pandas dataframe using mean/median or k-NN imputation.\n",
    "    \n",
    "    Parameters:\n",
    "    train_df (pd.DataFrame): dataframe containing training data with null values to be imputed.\n",
    "    test_df (pd.DataFrame): dataframe containing test data with null values to be imputed (default=None).\n",
    "                             If None, only the train set will be imputed.\n",
    "    method (str): 'mean' or 'median' for mean/median imputation or 'knn' for k-NN imputation (default='mean').\n",
    "    k (int): number of neighbors to use for k-NN imputation (default=5).\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: dataframe with imputed null values.\n",
    "    \"\"\"\n",
    "    # Make a copy of the original train dataframe to avoid modifying it.\n",
    "    imputed_train_df = train_df.copy()\n",
    "    if method is None:\n",
    "        return train_df, test_df\n",
    "    if method == 'mean':\n",
    "        # Replace null values with the mean of each column from the train set.\n",
    "        imputed_train_df.fillna(imputed_train_df.mean(), inplace=True)\n",
    "        \n",
    "    elif method == 'median':\n",
    "        # Replace null values with the median of each column from the train set.\n",
    "        imputed_train_df.fillna(imputed_train_df.median(), inplace=True)\n",
    "        \n",
    "    elif method == 'knn':\n",
    "        # Replace null values with k-NN imputation using the values from the train set.\n",
    "        imputer = KNNImputer(n_neighbors=k)\n",
    "        imputed_array = imputer.fit_transform(imputed_train_df)\n",
    "        imputed_train_df = pd.DataFrame(imputed_array, columns=imputed_train_df.columns)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Invalid imputation method. Choose either 'mean', 'median' or 'knn'.\")\n",
    "    \n",
    "    if test_df is not None:\n",
    "        # Make a copy of the original test dataframe to avoid modifying it.\n",
    "        imputed_test_df = test_df.copy()\n",
    "        \n",
    "        if method == 'mean':\n",
    "            # Replace null values with the mean of each column from the train set.\n",
    "            imputed_test_df.fillna(imputed_train_df.mean(), inplace=True)\n",
    "        \n",
    "        elif method == 'median':\n",
    "            # Replace null values with the median of each column from the train set.\n",
    "            imputed_test_df.fillna(imputed_train_df.median(), inplace=True)\n",
    "            \n",
    "        elif method == 'knn':\n",
    "            # Replace null values with k-NN imputation using the values from the train set.\n",
    "            imputed_array = imputer.transform(imputed_test_df)\n",
    "            imputed_test_df = pd.DataFrame(imputed_array, columns=imputed_test_df.columns)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Invalid imputation method. Choose either 'mean', 'median' or 'knn'.\")\n",
    "        \n",
    "        return imputed_train_df, imputed_test_df\n",
    "    \n",
    "    else:\n",
    "        return imputed_train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2558c4c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imp \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknn\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreparing dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     train_last_row \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows_to_include\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain...Done!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     test_last_row \u001b[38;5;241m=\u001b[39m prepare_df(test, rows_to_include\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mprepare_df\u001b[1;34m(df, rows_to_include)\u001b[0m\n\u001b[0;32m     13\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel_1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     14\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevel_1\u001b[39m\u001b[38;5;124m\"\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m df_last_row \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlevel_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows_to_include\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevel_0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSepsisLabel\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_last_row\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1414\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1412\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1413\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1414\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1415\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1416\u001b[0m         \u001b[38;5;66;03m# gh-20949\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m         \u001b[38;5;66;03m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1421\u001b[0m         \u001b[38;5;66;03m# fails on *some* columns, e.g. a numeric operation\u001b[39;00m\n\u001b[0;32m   1422\u001b[0m         \u001b[38;5;66;03m# on a string grouper column\u001b[39;00m\n\u001b[0;32m   1424\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group_selection_context():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1455\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same)\u001b[0m\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[0;32m   1431\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1434\u001b[0m     not_indexed_same: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1435\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   1436\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1437\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[0;32m   1438\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1453\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[0;32m   1454\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1455\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1458\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmutated\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:761\u001b[0m, in \u001b[0;36mBaseGrouper.apply\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[0;32m    760\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[1;32m--> 761\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[0;32m    763\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mprepare_df.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     13\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel_1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     14\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevel_1\u001b[39m\u001b[38;5;124m\"\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m df_last_row \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevel_0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mstats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows_to_include\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevel_0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSepsisLabel\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_last_row\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mprepare_df.<locals>.stats\u001b[1;34m(df, rows_to_include, future)\u001b[0m\n\u001b[0;32m      6\u001b[0m     index \u001b[38;5;241m=\u001b[39m df[df\u001b[38;5;241m.\u001b[39mSepsisLabel \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m     df \u001b[38;5;241m=\u001b[39m df[:index \u001b[38;5;241m+\u001b[39m future]\n\u001b[1;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtail\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows_to_include\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:11127\u001b[0m, in \u001b[0;36mNDFrame._add_numeric_operations.<locals>.mean\u001b[1;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11109\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[0;32m  11110\u001b[0m     _num_doc,\n\u001b[0;32m  11111\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn the mean of the values over the requested axis.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11125\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  11126\u001b[0m ):\n\u001b[1;32m> 11127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NDFrame\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28mself\u001b[39m, axis, skipna, level, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:10697\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[1;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  10689\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[0;32m  10690\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10691\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m lib\u001b[38;5;241m.\u001b[39mNoDefault \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mno_default,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10695\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  10696\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m> 10697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stat_function(\n\u001b[0;32m  10698\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, nanops\u001b[38;5;241m.\u001b[39mnanmean, axis, skipna, level, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m  10699\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:10649\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[1;34m(self, name, func, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  10639\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m  10640\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the level keyword in DataFrame and Series aggregations is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m  10641\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will be removed in a future version. Use groupby \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10644\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m  10645\u001b[0m     )\n\u001b[0;32m  10646\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_by_level(\n\u001b[0;32m  10647\u001b[0m         name, axis\u001b[38;5;241m=\u001b[39maxis, level\u001b[38;5;241m=\u001b[39mlevel, skipna\u001b[38;5;241m=\u001b[39mskipna, numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only\n\u001b[0;32m  10648\u001b[0m     )\n\u001b[1;32m> 10649\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\n\u001b[0;32m  10651\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:10008\u001b[0m, in \u001b[0;36mDataFrame._reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m  10004\u001b[0m ignore_failures \u001b[38;5;241m=\u001b[39m numeric_only \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m  10006\u001b[0m \u001b[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[0;32m  10007\u001b[0m \u001b[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[1;32m> 10008\u001b[0m res, _ \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_failures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_failures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10009\u001b[0m out \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_constructor(res)\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m  10010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1396\u001b[0m, in \u001b[0;36mBlockManager.reduce\u001b[1;34m(self, func, ignore_failures)\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res_blocks:\n\u001b[0;32m   1395\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([blk\u001b[38;5;241m.\u001b[39mmgr_locs\u001b[38;5;241m.\u001b[39mas_array \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m res_blocks])\n\u001b[1;32m-> 1396\u001b[0m     new_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_combine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1398\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\managers.py:558\u001b[0m, in \u001b[0;36mBaseBlockManager._combine\u001b[1;34m(self, blocks, copy, index)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_empty()\n\u001b[0;32m    557\u001b[0m \u001b[38;5;66;03m# FIXME: optimization potential\u001b[39;00m\n\u001b[1;32m--> 558\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmgr_locs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_array\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m inv_indexer \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mget_reverse_indexer(indexer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    561\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msort\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1004\u001b[0m, in \u001b[0;36msort\u001b[1;34m(a, axis, kind, order)\u001b[0m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1003\u001b[0m     a \u001b[38;5;241m=\u001b[39m asanyarray(a)\u001b[38;5;241m.\u001b[39mcopy(order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1004\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for imp in ['mean', 'median', 'knn']:\n",
    "    print(\"Preparing dataset...\")\n",
    "    train_last_row = prepare_df(train, rows_to_include=5)\n",
    "    print(\"Train...Done!\")\n",
    "    test_last_row = prepare_df(test, rows_to_include=5)\n",
    "    print(\"Test...Done!\")\n",
    "    print(\"Imputing train and test...\")\n",
    "    train_imp, test_imp = impute_null_values(train_last_row, test_last_row, imp, k=1)\n",
    "    print(\"Imputation Done!\")\n",
    "\n",
    "    X_train = train_imp.drop(\"label\", axis=1)\n",
    "    y_train = train_imp[\"label\"]\n",
    "    X_test = test_imp.drop(\"label\", axis=1)\n",
    "    y_test = test_imp[\"label\"]\n",
    "\n",
    "    def objective(trial):\n",
    "        # Define hyperparameters to be tuned\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 50, 1050, step=100)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 2, 32, log=True)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "        max_features = trial.suggest_categorical(\"max_features\", [\"auto\", \"sqrt\", \"log2\"])\n",
    "        sick_weight = trial.suggest_int(\"sick_weight\", 1, 10)\n",
    "        class_weights = {0: 1, 1: sick_weight}\n",
    "\n",
    "        # Define random forest classifier with hyperparameters\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            random_state=42,\n",
    "            class_weight=class_weights,\n",
    "        )\n",
    "\n",
    "        # Fit classifier to training data\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on validation data\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Calculate accuracy score\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        # Return accuracy score as objective value\n",
    "        return f1\n",
    "\n",
    "\n",
    "    # Define study object\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "    # Run hyperparameter optimization\n",
    "    study.optimize(objective, n_trials=30)\n",
    "    \n",
    "    class_weights = {0: 1, 1: study.best_params.pop('sick_weight')}\n",
    "    params = {k:v for k,v in study.best_params.items() if k != 'sick_weight'}\n",
    "    clf = RandomForestClassifier(**params, class_weight=class_weights)\n",
    "    clf.fit(X_train, y_train)\n",
    "    save_model(clf, imp+\"_RF\")\n",
    "    # Print best hyperparameters and objective value\n",
    "    print(\"Best hyperparameters: \", study.best_params)\n",
    "    print(\"Best f1: \", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fe60836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n",
      "Train...Done!\n",
      "Test...Done!\n",
      "Imputing train and test...\n",
      "Imputation Done!\n",
      "saved model mean\n",
      "Preparing dataset...\n",
      "Train...Done!\n",
      "Test...Done!\n",
      "Imputing train and test...\n",
      "Imputation Done!\n",
      "saved model median\n",
      "Preparing dataset...\n",
      "Train...Done!\n",
      "Test...Done!\n",
      "Imputing train and test...\n",
      "Imputation Done!\n",
      "saved model knn\n"
     ]
    }
   ],
   "source": [
    "mean_imp_best = {'n_estimators': 850, 'max_depth': 22, 'min_samples_split': 6, 'min_samples_leaf': 8, 'max_features': 'log2', 'sick_weight': 8}\n",
    "params = {k:v for k,v in mean_imp_best.items() if k != 'sick_weight'}\n",
    "median_imp_best = {'n_estimators': 350, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 8, 'max_features': 'log2', 'sick_weight': 5}\n",
    "params = {k:v for k,v in median_imp_best.items() if k != 'sick_weight'}\n",
    "knn_imp_best = {'n_estimators': 550, 'max_depth': 6, 'min_samples_split': 3, 'min_samples_leaf': 7, 'max_features': 'log2', 'sick_weight': 10}\n",
    "params = {k:v for k,v in knn_imp_best.items() if k != 'sick_weight'}\n",
    "\n",
    "for imp, params in [('mean', mean_imp_best), ('median', median_imp_best), ('knn', knn_imp_best)]:\n",
    "    class_weights = {0: 1, 1: params['sick_weight']}\n",
    "\n",
    "    params = {k:v for k,v in params.items() if k != 'sick_weight'}\n",
    "    \n",
    "    print(\"Preparing dataset...\")\n",
    "    train_last_row = prepare_df(train, rows_to_include=5)\n",
    "    print(\"Train...Done!\")\n",
    "    test_last_row = prepare_df(test, rows_to_include=5)\n",
    "    print(\"Test...Done!\")\n",
    "    print(\"Imputing train and test...\")\n",
    "    train_imp, test_imp = impute_null_values(train_last_row, test_last_row, imp, k=1)\n",
    "    print(\"Imputation Done!\")\n",
    "\n",
    "    X_train = train_imp.drop(\"label\", axis=1)\n",
    "    y_train = train_imp[\"label\"]\n",
    "    X_test = test_imp.drop(\"label\", axis=1)\n",
    "    y_test = test_imp[\"label\"]\n",
    "    clf = RandomForestClassifier(**params, class_weight=class_weights)\n",
    "    clf.fit(X_train, y_train)\n",
    "    save_model(clf, imp+\"_RF\")\n",
    "    print(f\"saved model {imp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79ea7bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n",
      "Train...Done!\n",
      "Test...Done!\n",
      "Imputing train and test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-09 13:53:00,526]\u001b[0m A new study created in memory with name: no-name-44fc3536-c6dc-411f-beb3-c69d6179f119\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-09 13:53:21,787]\u001b[0m Trial 0 finished with value: 0.4911955514365152 and parameters: {'n_estimators': 465, 'learning_rate': 0.0011770125849433326, 'algorithm': 'SAMME', 'random_state': 47}. Best is trial 0 with value: 0.4911955514365152.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 13:53:40,503]\u001b[0m Trial 1 finished with value: 0.4911955514365152 and parameters: {'n_estimators': 376, 'learning_rate': 0.003179947024878677, 'algorithm': 'SAMME', 'random_state': 70}. Best is trial 0 with value: 0.4911955514365152.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 13:54:01,737]\u001b[0m Trial 2 finished with value: 0.4911955514365152 and parameters: {'n_estimators': 254, 'learning_rate': 0.014386850372288278, 'algorithm': 'SAMME', 'random_state': 44}. Best is trial 0 with value: 0.4911955514365152.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 13:54:35,487]\u001b[0m Trial 3 finished with value: 0.6570281124497992 and parameters: {'n_estimators': 410, 'learning_rate': 0.42219065420044877, 'algorithm': 'SAMME.R', 'random_state': 37}. Best is trial 3 with value: 0.6570281124497992.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 13:54:43,255]\u001b[0m Trial 4 finished with value: 0.5464382326420197 and parameters: {'n_estimators': 100, 'learning_rate': 0.7956345957787144, 'algorithm': 'SAMME', 'random_state': 46}. Best is trial 3 with value: 0.6570281124497992.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 13:55:05,058]\u001b[0m Trial 5 finished with value: 0.39959016393442626 and parameters: {'n_estimators': 382, 'learning_rate': 0.013321167852724532, 'algorithm': 'SAMME', 'random_state': 23}. Best is trial 3 with value: 0.6570281124497992.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 13:55:18,288]\u001b[0m Trial 6 finished with value: 0.4911955514365152 and parameters: {'n_estimators': 295, 'learning_rate': 0.00677920736640716, 'algorithm': 'SAMME', 'random_state': 58}. Best is trial 3 with value: 0.6570281124497992.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 13:55:40,857]\u001b[0m Trial 7 finished with value: 0.4911955514365152 and parameters: {'n_estimators': 364, 'learning_rate': 0.0035403543285161456, 'algorithm': 'SAMME.R', 'random_state': 28}. Best is trial 3 with value: 0.6570281124497992.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 13:56:17,061]\u001b[0m Trial 8 finished with value: 0.5580985915492956 and parameters: {'n_estimators': 470, 'learning_rate': 0.03331488524030093, 'algorithm': 'SAMME.R', 'random_state': 72}. Best is trial 3 with value: 0.6570281124497992.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 13:56:57,267]\u001b[0m Trial 9 finished with value: 0.656025538707103 and parameters: {'n_estimators': 439, 'learning_rate': 0.4597677877146156, 'algorithm': 'SAMME.R', 'random_state': 48}. Best is trial 3 with value: 0.6570281124497992.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 13:57:13,435]\u001b[0m Trial 10 finished with value: 0.6182432432432432 and parameters: {'n_estimators': 183, 'learning_rate': 0.1686633220472883, 'algorithm': 'SAMME.R', 'random_state': 98}. Best is trial 3 with value: 0.6570281124497992.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 13:57:52,628]\u001b[0m Trial 11 finished with value: 0.6433121019108281 and parameters: {'n_estimators': 499, 'learning_rate': 0.8702533139328681, 'algorithm': 'SAMME.R', 'random_state': 1}. Best is trial 3 with value: 0.6570281124497992.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 13:58:29,237]\u001b[0m Trial 12 finished with value: 0.6504460665044607 and parameters: {'n_estimators': 424, 'learning_rate': 0.19794940080751636, 'algorithm': 'SAMME.R', 'random_state': 25}. Best is trial 3 with value: 0.6570281124497992.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 13:58:46,781]\u001b[0m Trial 13 finished with value: 0.6525630593978845 and parameters: {'n_estimators': 311, 'learning_rate': 0.21897261907719487, 'algorithm': 'SAMME.R', 'random_state': 8}. Best is trial 3 with value: 0.6570281124497992.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 13:59:11,747]\u001b[0m Trial 14 finished with value: 0.6098807495741057 and parameters: {'n_estimators': 421, 'learning_rate': 0.06654014415900443, 'algorithm': 'SAMME.R', 'random_state': 36}. Best is trial 3 with value: 0.6570281124497992.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'n_estimators': 410, 'learning_rate': 0.42219065420044877, 'algorithm': 'SAMME.R', 'random_state': 37}\n",
      "Best f1 score:  0.6570281124497992\n",
      "Preparing dataset...\n",
      "Train...Done!\n",
      "Test...Done!\n",
      "Imputing train and test...\n",
      "Imputation Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-09 14:00:38,206]\u001b[0m A new study created in memory with name: no-name-0befeb8e-dbab-498b-a265-0a442207d524\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:00:49,160]\u001b[0m Trial 0 finished with value: 0.4139387539598733 and parameters: {'n_estimators': 184, 'learning_rate': 0.012966626035152036, 'algorithm': 'SAMME', 'random_state': 52}. Best is trial 0 with value: 0.4139387539598733.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:00:52,357]\u001b[0m Trial 1 finished with value: 0.4911955514365152 and parameters: {'n_estimators': 53, 'learning_rate': 0.012930596039955283, 'algorithm': 'SAMME', 'random_state': 40}. Best is trial 1 with value: 0.4911955514365152.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:01:05,073]\u001b[0m Trial 2 finished with value: 0.4911955514365152 and parameters: {'n_estimators': 184, 'learning_rate': 0.0038293197155126924, 'algorithm': 'SAMME', 'random_state': 3}. Best is trial 1 with value: 0.4911955514365152.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:01:42,384]\u001b[0m Trial 3 finished with value: 0.6167097329888028 and parameters: {'n_estimators': 464, 'learning_rate': 0.33169795105765804, 'algorithm': 'SAMME', 'random_state': 55}. Best is trial 3 with value: 0.6167097329888028.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:02:15,835]\u001b[0m Trial 4 finished with value: 0.5860547219770522 and parameters: {'n_estimators': 438, 'learning_rate': 0.03586483518106894, 'algorithm': 'SAMME.R', 'random_state': 66}. Best is trial 3 with value: 0.6167097329888028.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:02:52,578]\u001b[0m Trial 5 finished with value: 0.4911955514365152 and parameters: {'n_estimators': 474, 'learning_rate': 0.0017993542535463633, 'algorithm': 'SAMME.R', 'random_state': 10}. Best is trial 3 with value: 0.6167097329888028.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:03:10,694]\u001b[0m Trial 6 finished with value: 0.3499446290143965 and parameters: {'n_estimators': 251, 'learning_rate': 0.03025425314631262, 'algorithm': 'SAMME', 'random_state': 52}. Best is trial 3 with value: 0.6167097329888028.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:03:42,227]\u001b[0m Trial 7 finished with value: 0.41561181434599165 and parameters: {'n_estimators': 430, 'learning_rate': 0.029305638765611274, 'algorithm': 'SAMME', 'random_state': 6}. Best is trial 3 with value: 0.6167097329888028.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:03:56,319]\u001b[0m Trial 8 finished with value: 0.4911955514365152 and parameters: {'n_estimators': 181, 'learning_rate': 0.001097253812358414, 'algorithm': 'SAMME.R', 'random_state': 1}. Best is trial 3 with value: 0.6167097329888028.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:04:24,726]\u001b[0m Trial 9 finished with value: 0.4222689075630252 and parameters: {'n_estimators': 390, 'learning_rate': 0.04323913493532075, 'algorithm': 'SAMME', 'random_state': 72}. Best is trial 3 with value: 0.6167097329888028.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:04:50,615]\u001b[0m Trial 10 finished with value: 0.6640190627482129 and parameters: {'n_estimators': 331, 'learning_rate': 0.580222095689923, 'algorithm': 'SAMME.R', 'random_state': 96}. Best is trial 10 with value: 0.6640190627482129.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:05:18,180]\u001b[0m Trial 11 finished with value: 0.6792452830188679 and parameters: {'n_estimators': 357, 'learning_rate': 0.6618631252661278, 'algorithm': 'SAMME.R', 'random_state': 95}. Best is trial 11 with value: 0.6792452830188679.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:05:42,841]\u001b[0m Trial 12 finished with value: 0.658326817826427 and parameters: {'n_estimators': 339, 'learning_rate': 0.8643507726041687, 'algorithm': 'SAMME.R', 'random_state': 100}. Best is trial 11 with value: 0.6792452830188679.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:06:04,866]\u001b[0m Trial 13 finished with value: 0.6704270749395649 and parameters: {'n_estimators': 321, 'learning_rate': 0.27537884633981224, 'algorithm': 'SAMME.R', 'random_state': 96}. Best is trial 11 with value: 0.6792452830188679.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:06:24,440]\u001b[0m Trial 14 finished with value: 0.6623376623376623 and parameters: {'n_estimators': 281, 'learning_rate': 0.202575714866453, 'algorithm': 'SAMME.R', 'random_state': 81}. Best is trial 11 with value: 0.6792452830188679.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'n_estimators': 357, 'learning_rate': 0.6618631252661278, 'algorithm': 'SAMME.R', 'random_state': 95}\n",
      "Best f1 score:  0.6792452830188679\n",
      "Preparing dataset...\n",
      "Train...Done!\n",
      "Test...Done!\n",
      "Imputing train and test...\n",
      "Imputation Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-09 14:07:41,130]\u001b[0m A new study created in memory with name: no-name-a4919313-461a-4069-8f1c-52990150d443\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:08:08,046]\u001b[0m Trial 0 finished with value: 0.4911955514365152 and parameters: {'n_estimators': 327, 'learning_rate': 0.0010055281423484227, 'algorithm': 'SAMME.R', 'random_state': 71}. Best is trial 0 with value: 0.4911955514365152.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:08:37,080]\u001b[0m Trial 1 finished with value: 0.6265664160401003 and parameters: {'n_estimators': 354, 'learning_rate': 0.4537209174164803, 'algorithm': 'SAMME', 'random_state': 42}. Best is trial 1 with value: 0.6265664160401003.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:09:12,523]\u001b[0m Trial 2 finished with value: 0.4975798644724105 and parameters: {'n_estimators': 434, 'learning_rate': 0.01869315268245823, 'algorithm': 'SAMME.R', 'random_state': 76}. Best is trial 1 with value: 0.6265664160401003.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:09:19,754]\u001b[0m Trial 3 finished with value: 0.6502866502866502 and parameters: {'n_estimators': 100, 'learning_rate': 0.4041774745514522, 'algorithm': 'SAMME.R', 'random_state': 58}. Best is trial 3 with value: 0.6502866502866502.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:10:03,008]\u001b[0m Trial 4 finished with value: 0.4911955514365152 and parameters: {'n_estimators': 499, 'learning_rate': 0.0023985334994794465, 'algorithm': 'SAMME.R', 'random_state': 18}. Best is trial 3 with value: 0.6502866502866502.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:10:35,876]\u001b[0m Trial 5 finished with value: 0.4785643070787637 and parameters: {'n_estimators': 405, 'learning_rate': 0.06719511531599628, 'algorithm': 'SAMME', 'random_state': 9}. Best is trial 3 with value: 0.6502866502866502.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:10:40,467]\u001b[0m Trial 6 finished with value: 0.4911955514365152 and parameters: {'n_estimators': 55, 'learning_rate': 0.00527644757860739, 'algorithm': 'SAMME', 'random_state': 51}. Best is trial 3 with value: 0.6502866502866502.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:10:48,125]\u001b[0m Trial 7 finished with value: 0.4361370716510904 and parameters: {'n_estimators': 95, 'learning_rate': 0.017736308027005966, 'algorithm': 'SAMME', 'random_state': 89}. Best is trial 3 with value: 0.6502866502866502.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:11:06,888]\u001b[0m Trial 8 finished with value: 0.43124999999999997 and parameters: {'n_estimators': 211, 'learning_rate': 0.01850989774235312, 'algorithm': 'SAMME.R', 'random_state': 8}. Best is trial 3 with value: 0.6502866502866502.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:11:41,622]\u001b[0m Trial 9 finished with value: 0.38177874186550975 and parameters: {'n_estimators': 419, 'learning_rate': 0.007872789850855351, 'algorithm': 'SAMME', 'random_state': 80}. Best is trial 3 with value: 0.6502866502866502.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:11:57,542]\u001b[0m Trial 10 finished with value: 0.671383647798742 and parameters: {'n_estimators': 181, 'learning_rate': 0.840963198000444, 'algorithm': 'SAMME.R', 'random_state': 50}. Best is trial 10 with value: 0.671383647798742.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:12:12,977]\u001b[0m Trial 11 finished with value: 0.6735015772870663 and parameters: {'n_estimators': 174, 'learning_rate': 0.9761878568519904, 'algorithm': 'SAMME.R', 'random_state': 52}. Best is trial 11 with value: 0.6735015772870663.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:12:27,952]\u001b[0m Trial 12 finished with value: 0.6520031421838177 and parameters: {'n_estimators': 205, 'learning_rate': 0.9861200102774433, 'algorithm': 'SAMME.R', 'random_state': 37}. Best is trial 11 with value: 0.6735015772870663.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:12:41,680]\u001b[0m Trial 13 finished with value: 0.6221470836855454 and parameters: {'n_estimators': 196, 'learning_rate': 0.13325481671124842, 'algorithm': 'SAMME.R', 'random_state': 30}. Best is trial 11 with value: 0.6735015772870663.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:13:04,524]\u001b[0m Trial 14 finished with value: 0.6447368421052632 and parameters: {'n_estimators': 259, 'learning_rate': 0.18029046565375417, 'algorithm': 'SAMME.R', 'random_state': 61}. Best is trial 11 with value: 0.6735015772870663.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'n_estimators': 174, 'learning_rate': 0.9761878568519904, 'algorithm': 'SAMME.R', 'random_state': 52}\n",
      "Best f1 score:  0.6735015772870663\n"
     ]
    }
   ],
   "source": [
    "for rows in [1,5,10]:\n",
    "    print(\"Preparing dataset...\")\n",
    "    train_last_row = prepare_df(train, rows_to_include=rows)\n",
    "    print(\"Train...Done!\")\n",
    "    test_last_row = prepare_df(test, rows_to_include=rows)\n",
    "    print(\"Test...Done!\")\n",
    "    print(\"Imputing train and test...\")\n",
    "    train_imp, test_imp = impute_null_values(train_last_row, test_last_row, 'mean', k=1)\n",
    "    print(\"Imputation Done!\")\n",
    "    X_train = train_imp.drop(columns=[\"label\"], axis=1)\n",
    "    y_train = train_imp[\"label\"]\n",
    "    X_test = test_imp.drop(columns=[\"label\"], axis=1)\n",
    "    y_test = test_imp[\"label\"]\n",
    "\n",
    "    def objective(trial):\n",
    "        # Define hyperparameters\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 1.0)\n",
    "        algorithm = trial.suggest_categorical('algorithm', ['SAMME', 'SAMME.R'])\n",
    "        random_state = trial.suggest_int('random_state', 0, 100)\n",
    "\n",
    "        # Train Adaboost classifier with the given hyperparameters\n",
    "        model = AdaBoostClassifier(n_estimators=n_estimators, learning_rate=learning_rate, \n",
    "                                   algorithm=algorithm, random_state=random_state)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the model on the testing set\n",
    "        y_pred = model.predict(X_test)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        return f1\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=15)\n",
    "    \n",
    "    model = AdaBoostClassifier(**study.best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    save_model(model, f\"rows_{rows}_ADA\")\n",
    "    # Print the best hyperparameters and f1 score\n",
    "    print(\"Best hyperparameters: \", study.best_params)\n",
    "    print(\"Best f1 score: \", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763e19cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
